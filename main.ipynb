{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OASIS Cross-Sectional MRI Dataset to DataFrame Converter\n",
    "\n",
    "This notebook processes the OASIS dataset and converts it into pandas DataFrames for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_file(txt_path):\n",
    "    \"\"\"Parse the .txt file to extract subject metadata.\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    with open(txt_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    patterns = {\n",
    "        'SESSION_ID': r'SESSION ID:\\s+(.+)',\n",
    "        'AGE': r'AGE:\\s+(\\d+)',\n",
    "        'M/F': r'M/F:\\s+(\\w+)',\n",
    "        'HAND': r'HAND:\\s+(\\w+)',\n",
    "        'EDUC': r'EDUC:\\s+(\\d+)',\n",
    "        'SES': r'SES:\\s+(\\d+)',\n",
    "        'CDR': r'CDR:\\s+([\\d.]+)',\n",
    "        'MMSE': r'MMSE:\\s+(\\d+)',\n",
    "        'eTIV': r'eTIV:\\s+([\\d.]+)',\n",
    "        'ASF': r'ASF:\\s+([\\d.]+)',\n",
    "        'nWBV': r'nWBV:\\s+([\\d.]+)'\n",
    "    }\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, content)\n",
    "        if match:\n",
    "            data[key] = match.group(1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_file(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    ns = {'xnat': 'http://nrg.wustl.edu/xnat',\n",
    "          'oasis': 'http://nmr.mgh.harvard.edu/oasis'}\n",
    "    \n",
    "    data = {\n",
    "        'session_id': root.get('ID'),\n",
    "        'subject_id': root.find('xnat:subject_ID', ns).text if root.find('xnat:subject_ID', ns) is not None else None,\n",
    "        'scans': [],\n",
    "        'reconstructions': [],\n",
    "        'assessors': []\n",
    "    }\n",
    "    \n",
    "    scans = root.find('xnat:scans', ns)\n",
    "    if scans is not None:\n",
    "        for scan in scans.findall('xnat:scan', ns):\n",
    "            scan_data = {\n",
    "                'scan_id': scan.get('ID'),\n",
    "                'type': scan.get('type'),\n",
    "                'quality': scan.find('xnat:quality', ns).text if scan.find('xnat:quality', ns) is not None else None\n",
    "            }\n",
    "            \n",
    "            # Parse parameters\n",
    "            params = scan.find('xnat:parameters', ns)\n",
    "            if params is not None:\n",
    "                voxel_res = params.find('xnat:voxelRes', ns)\n",
    "                if voxel_res is not None:\n",
    "                    scan_data['voxel_x'] = voxel_res.get('x')\n",
    "                    scan_data['voxel_y'] = voxel_res.get('y')\n",
    "                    scan_data['voxel_z'] = voxel_res.get('z')\n",
    "                \n",
    "                scan_data['orientation'] = params.find('xnat:orientation', ns).text if params.find('xnat:orientation', ns) is not None else None\n",
    "                scan_data['tr'] = params.find('xnat:tr', ns).text if params.find('xnat:tr', ns) is not None else None\n",
    "                scan_data['te'] = params.find('xnat:te', ns).text if params.find('xnat:te', ns) is not None else None\n",
    "                scan_data['ti'] = params.find('xnat:ti', ns).text if params.find('xnat:ti', ns) is not None else None\n",
    "                scan_data['flip'] = params.find('xnat:flip', ns).text if params.find('xnat:flip', ns) is not None else None\n",
    "            \n",
    "            data['scans'].append(scan_data)\n",
    "    \n",
    "    # Parse assessors\n",
    "    assessors = root.find('xnat:assessors', ns)\n",
    "    if assessors is not None:\n",
    "        for assessor in assessors.findall('xnat:assessor', ns):\n",
    "            assessor_type = assessor.get('{http://www.w3.org/2001/XMLSchema-instance}type')\n",
    "            assessor_data = {\n",
    "                'assessor_id': assessor.get('ID'),\n",
    "                'type': assessor_type\n",
    "            }\n",
    "            \n",
    "            # Atlas Scaling Factor\n",
    "            if 'atlasScalingFactorData' in assessor_type:\n",
    "                scaling_factor = assessor.find('oasis:scalingFactor', ns)\n",
    "                eicv = assessor.find('oasis:eICV', ns)\n",
    "                if scaling_factor is not None:\n",
    "                    assessor_data['scaling_factor'] = scaling_factor.text\n",
    "                if eicv is not None:\n",
    "                    assessor_data['eICV'] = eicv.text\n",
    "            \n",
    "            # Segmentation data\n",
    "            if 'segmentationFastData' in assessor_type:\n",
    "                brain_percent = assessor.get('brainPercent')\n",
    "                if brain_percent:\n",
    "                    assessor_data['brain_percent'] = brain_percent\n",
    "            \n",
    "            data['assessors'].append(assessor_data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process All Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_oasis_dataset(data_dir):\n",
    "    data_path = Path(data_dir)\n",
    "    subjects_data = []\n",
    "    scans_data = []\n",
    "    assessors_data = []\n",
    "    \n",
    "    # Check if path exists\n",
    "    if not data_path.exists():\n",
    "        print(f\"Error: {data_path} does not exist!\")\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # Get all top-level directories\n",
    "    top_level_dirs = [d for d in data_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(top_level_dirs)} top-level directories\")\n",
    "    \n",
    "    for top_dir in top_level_dirs:\n",
    "        print(f\"\\nProcessing {top_dir.name}...\")\n",
    "        \n",
    "        # Look for 'disc1', 'disc2', etc. folders inside\n",
    "        disc_folders = [d for d in top_dir.iterdir() if d.is_dir() and d.name.lower().startswith('disc')]\n",
    "        \n",
    "        if not disc_folders:\n",
    "            # Maybe subjects are directly in this folder?\n",
    "            subject_dirs = [d for d in top_dir.iterdir() if d.is_dir() and d.name.startswith('OAS1_')]\n",
    "            if subject_dirs:\n",
    "                print(f\"  Found {len(subject_dirs)} subjects directly in {top_dir.name}\")\n",
    "                disc_folders = [top_dir]  # Process this directory as if it were a disc folder\n",
    "            else:\n",
    "                print(f\"  No disc folders or subjects found in {top_dir.name}\")\n",
    "                continue\n",
    "        else:\n",
    "            print(f\"  Found {len(disc_folders)} disc folders\")\n",
    "        \n",
    "        for disc_folder in disc_folders:\n",
    "            # Find all subject directories (OAS1_XXXX_MR1)\n",
    "            subject_dirs = [d for d in disc_folder.iterdir() if d.is_dir() and d.name.startswith('OAS1_')]\n",
    "            print(f\"    Processing {disc_folder.name}: {len(subject_dirs)} subjects\")\n",
    "            \n",
    "            for subject_dir in subject_dirs:\n",
    "                subject_id = subject_dir.name\n",
    "                txt_file = subject_dir / f\"{subject_id}.txt\"\n",
    "                xml_file = subject_dir / f\"{subject_id}.xml\"\n",
    "                \n",
    "                if txt_file.exists() and xml_file.exists():\n",
    "                    try:\n",
    "                        txt_data = parse_txt_file(txt_file)\n",
    "                        xml_data = parse_xml_file(xml_file)\n",
    "                        \n",
    "                        subject_record = {**txt_data, 'disc': top_dir.name}\n",
    "                        subjects_data.append(subject_record)\n",
    "                        \n",
    "                        for scan in xml_data['scans']:\n",
    "                            scan_record = {'subject_id': subject_id, **scan}\n",
    "                            scans_data.append(scan_record)\n",
    "                        \n",
    "                        for assessor in xml_data['assessors']:\n",
    "                            assessor_record = {'subject_id': subject_id, **assessor}\n",
    "                            assessors_data.append(assessor_record)\n",
    "                    except Exception as e:\n",
    "                        print(f\"      Error processing {subject_id}: {e}\")\n",
    "                else:\n",
    "                    if not txt_file.exists():\n",
    "                        print(f\"      Missing: {txt_file.name}\")\n",
    "                    if not xml_file.exists():\n",
    "                        print(f\"      Missing: {xml_file.name}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Total subjects processed: {len(subjects_data)}\")\n",
    "    print(f\"Total scans: {len(scans_data)}\")\n",
    "    print(f\"Total assessors: {len(assessors_data)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df_subjects = pd.DataFrame(subjects_data)\n",
    "    df_scans = pd.DataFrame(scans_data)\n",
    "    df_assessors = pd.DataFrame(assessors_data)\n",
    "    \n",
    "    return df_subjects, df_scans, df_assessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing OASIS Cross-Sectional Disc 1...\n",
      "Processing Oasis Cross-Sectional Disc 10...\n",
      "Processing Oasis Cross-Sectional Disc 11...\n",
      "Processing Oasis Cross Sectional Disc 12...\n",
      "Processing Oasis Cross-Sectional Disc 2...\n",
      "Processing Oasis Cross-Sectional Disc 3...\n",
      "Processing Oasis Cross-Sectional Disc 4...\n",
      "Processing OASIS Cross Sectional Disc 5...\n",
      "Processing Oasis Cross-Sectional Disc 6...\n",
      "Processing Oasis Cross-Sectional Disc 7...\n",
      "Processing Oasis Cross-Sectional Disc 8...\n",
      "Processing Oasis Cross-Sectional Disc 9...\n",
      "\n",
      "Processing complete!\n",
      "Subjects: 436 records\n",
      "Scans: 1688 records\n",
      "Assessors: 872 records\n"
     ]
    }
   ],
   "source": [
    "# Set the data directory path\n",
    "data_dir = './data'\n",
    "\n",
    "# Process the dataset\n",
    "df_subjects, df_scans, df_assessors = process_oasis_dataset(data_dir)\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Subjects: {len(df_subjects)} records\")\n",
    "print(f\"Scans: {len(df_scans)} records\")\n",
    "print(f\"Assessors: {len(df_assessors)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUBJECTS DATAFRAME ===\n",
      "      SESSION_ID AGE     M/F   HAND EDUC  SES  CDR MMSE     eTIV   ASF   nWBV  \\\n",
      "0  OAS1_0001_MR1  74  Female  Right    2    3    0   29  1344.00  1.31  0.743   \n",
      "1  OAS1_0002_MR1  55  Female  Right    4    1    0   29  1147.00  1.53  0.810   \n",
      "2  OAS1_0003_MR1  73  Female  Right    4    3  0.5   27  1454.00  1.21  0.708   \n",
      "3  OAS1_0004_MR1  28    Male  Right  NaN  NaN  NaN  NaN  1588.00  1.11  0.803   \n",
      "4  OAS1_0005_MR1  18    Male  Right  NaN  NaN  NaN  NaN  1737.00  1.01  0.848   \n",
      "\n",
      "                           disc  \n",
      "0  OASIS Cross-Sectional Disc 1  \n",
      "1  OASIS Cross-Sectional Disc 1  \n",
      "2  OASIS Cross-Sectional Disc 1  \n",
      "3  OASIS Cross-Sectional Disc 1  \n",
      "4  OASIS Cross-Sectional Disc 1  \n",
      "\n",
      "Shape: (436, 12)\n",
      "\n",
      "Columns: ['SESSION_ID', 'AGE', 'M/F', 'HAND', 'EDUC', 'SES', 'CDR', 'MMSE', 'eTIV', 'ASF', 'nWBV', 'disc']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SUBJECTS DATAFRAME ===\")\n",
    "print(df_subjects.head())\n",
    "print(f\"\\nShape: {df_subjects.shape}\")\n",
    "print(f\"\\nColumns: {df_subjects.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      subject_id scan_id    type quality voxel_x voxel_y voxel_z orientation  \\\n",
      "0  OAS1_0001_MR1   mpr-1  MPRAGE  usable     1.0     1.0    1.25         Sag   \n",
      "1  OAS1_0001_MR1   mpr-2  MPRAGE  usable     1.0     1.0    1.25         Sag   \n",
      "2  OAS1_0001_MR1   mpr-3  MPRAGE  usable     1.0     1.0    1.25         Sag   \n",
      "3  OAS1_0001_MR1   mpr-4  MPRAGE  usable     1.0     1.0    1.25         Sag   \n",
      "4  OAS1_0002_MR1   mpr-1  MPRAGE  usable     1.0     1.0    1.25         Sag   \n",
      "\n",
      "    tr   te    ti flip  \n",
      "0  9.7  4.0  20.0   10  \n",
      "1  9.7  4.0  20.0   10  \n",
      "2  9.7  4.0  20.0   10  \n",
      "3  9.7  4.0  20.0   10  \n",
      "4  9.7  4.0  20.0   10  \n",
      "\n",
      "Shape: (1688, 12)\n",
      "\n",
      "Columns: ['subject_id', 'scan_id', 'type', 'quality', 'voxel_x', 'voxel_y', 'voxel_z', 'orientation', 'tr', 'te', 'ti', 'flip']\n"
     ]
    }
   ],
   "source": [
    "print(df_scans.head())\n",
    "print(f\"\\nShape: {df_scans.shape}\")\n",
    "print(f\"\\nColumns: {df_scans.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Type Conversion and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types converted successfully!\n"
     ]
    }
   ],
   "source": [
    "numeric_cols = ['AGE', 'EDUC', 'SES', 'CDR', 'MMSE', 'eTIV', 'ASF', 'nWBV']\n",
    "for col in numeric_cols:\n",
    "    if col in df_subjects.columns:\n",
    "        df_subjects[col] = pd.to_numeric(df_subjects[col], errors='coerce')\n",
    "\n",
    "scan_numeric_cols = ['voxel_x', 'voxel_y', 'voxel_z', 'tr', 'te', 'ti', 'flip']\n",
    "for col in scan_numeric_cols:\n",
    "    if col in df_scans.columns:\n",
    "        df_scans[col] = pd.to_numeric(df_scans[col], errors='coerce')\n",
    "\n",
    "assessor_numeric_cols = ['scaling_factor', 'eICV', 'brain_percent']\n",
    "for col in assessor_numeric_cols:\n",
    "    if col in df_assessors.columns:\n",
    "        df_assessors[col] = pd.to_numeric(df_assessors[col], errors='coerce')\n",
    "\n",
    "print(\"Data types converted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SUBJECT DEMOGRAPHICS ===\n",
      "\n",
      "Age statistics:\n",
      "count    436.000000\n",
      "mean      51.357798\n",
      "std       25.269862\n",
      "min       18.000000\n",
      "25%       23.000000\n",
      "50%       54.000000\n",
      "75%       74.000000\n",
      "max       96.000000\n",
      "Name: AGE, dtype: float64\n",
      "\n",
      "Gender distribution:\n",
      "M/F\n",
      "Female    268\n",
      "Male      168\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CDR (Clinical Dementia Rating) distribution:\n",
      "CDR\n",
      "0.0    135\n",
      "0.5     70\n",
      "1.0     28\n",
      "2.0      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SUBJECT DEMOGRAPHICS ===\")\n",
    "print(f\"\\nAge statistics:\")\n",
    "print(df_subjects['AGE'].describe())\n",
    "\n",
    "print(f\"\\nGender distribution:\")\n",
    "print(df_subjects['M/F'].value_counts())\n",
    "\n",
    "print(f\"\\nCDR (Clinical Dementia Rating) distribution:\")\n",
    "print(df_subjects['CDR'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save DataFrames to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrames saved to CSV files:\n",
      "- oasis_subjects.csv\n",
      "- oasis_scans.csv\n",
      "- oasis_assessors.csv\n"
     ]
    }
   ],
   "source": [
    "df_subjects.to_csv('oasis_subjects.csv', index=False)\n",
    "df_scans.to_csv('oasis_scans.csv', index=False)\n",
    "df_assessors.to_csv('oasis_assessors.csv', index=False)\n",
    "\n",
    "print(\"\\nDataFrames saved to CSV files:\")\n",
    "print(\"- oasis_subjects.csv\")\n",
    "print(\"- oasis_scans.csv\")\n",
    "print(\"- oasis_assessors.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
