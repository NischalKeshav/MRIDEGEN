{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OASIS Cross-Sectional MRI Dataset to DataFrame Converter\n",
    "\n",
    "This notebook processes the OASIS dataset and converts it into pandas DataFrames for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_txt_file(txt_path):\n",
    "    \"\"\"Parse the .txt file to extract subject metadata.\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    with open(txt_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract basic info\n",
    "    patterns = {\n",
    "        'SESSION_ID': r'SESSION ID:\\s+(.+)',\n",
    "        'AGE': r'AGE:\\s+(\\d+)',\n",
    "        'M/F': r'M/F:\\s+(\\w+)',\n",
    "        'HAND': r'HAND:\\s+(\\w+)',\n",
    "        'EDUC': r'EDUC:\\s+(\\d+)',\n",
    "        'SES': r'SES:\\s+(\\d+)',\n",
    "        'CDR': r'CDR:\\s+([\\d.]+)',\n",
    "        'MMSE': r'MMSE:\\s+(\\d+)',\n",
    "        'eTIV': r'eTIV:\\s+([\\d.]+)',\n",
    "        'ASF': r'ASF:\\s+([\\d.]+)',\n",
    "        'nWBV': r'nWBV:\\s+([\\d.]+)'\n",
    "    }\n",
    "    \n",
    "    for key, pattern in patterns.items():\n",
    "        match = re.search(pattern, content)\n",
    "        if match:\n",
    "            data[key] = match.group(1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_xml_file(xml_path):\n",
    "    \"\"\"Parse the .xml file to extract scan and processing information.\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    # Define namespace\n",
    "    ns = {'xnat': 'http://nrg.wustl.edu/xnat',\n",
    "          'oasis': 'http://nmr.mgh.harvard.edu/oasis'}\n",
    "    \n",
    "    data = {\n",
    "        'session_id': root.get('ID'),\n",
    "        'subject_id': root.find('xnat:subject_ID', ns).text if root.find('xnat:subject_ID', ns) is not None else None,\n",
    "        'scans': [],\n",
    "        'reconstructions': [],\n",
    "        'assessors': []\n",
    "    }\n",
    "    \n",
    "    # Parse scans\n",
    "    scans = root.find('xnat:scans', ns)\n",
    "    if scans is not None:\n",
    "        for scan in scans.findall('xnat:scan', ns):\n",
    "            scan_data = {\n",
    "                'scan_id': scan.get('ID'),\n",
    "                'type': scan.get('type'),\n",
    "                'quality': scan.find('xnat:quality', ns).text if scan.find('xnat:quality', ns) is not None else None\n",
    "            }\n",
    "            \n",
    "            # Parse parameters\n",
    "            params = scan.find('xnat:parameters', ns)\n",
    "            if params is not None:\n",
    "                voxel_res = params.find('xnat:voxelRes', ns)\n",
    "                if voxel_res is not None:\n",
    "                    scan_data['voxel_x'] = voxel_res.get('x')\n",
    "                    scan_data['voxel_y'] = voxel_res.get('y')\n",
    "                    scan_data['voxel_z'] = voxel_res.get('z')\n",
    "                \n",
    "                scan_data['orientation'] = params.find('xnat:orientation', ns).text if params.find('xnat:orientation', ns) is not None else None\n",
    "                scan_data['tr'] = params.find('xnat:tr', ns).text if params.find('xnat:tr', ns) is not None else None\n",
    "                scan_data['te'] = params.find('xnat:te', ns).text if params.find('xnat:te', ns) is not None else None\n",
    "                scan_data['ti'] = params.find('xnat:ti', ns).text if params.find('xnat:ti', ns) is not None else None\n",
    "                scan_data['flip'] = params.find('xnat:flip', ns).text if params.find('xnat:flip', ns) is not None else None\n",
    "            \n",
    "            data['scans'].append(scan_data)\n",
    "    \n",
    "    # Parse assessors\n",
    "    assessors = root.find('xnat:assessors', ns)\n",
    "    if assessors is not None:\n",
    "        for assessor in assessors.findall('xnat:assessor', ns):\n",
    "            assessor_type = assessor.get('{http://www.w3.org/2001/XMLSchema-instance}type')\n",
    "            assessor_data = {\n",
    "                'assessor_id': assessor.get('ID'),\n",
    "                'type': assessor_type\n",
    "            }\n",
    "            \n",
    "            # Atlas Scaling Factor\n",
    "            if 'atlasScalingFactorData' in assessor_type:\n",
    "                scaling_factor = assessor.find('oasis:scalingFactor', ns)\n",
    "                eicv = assessor.find('oasis:eICV', ns)\n",
    "                if scaling_factor is not None:\n",
    "                    assessor_data['scaling_factor'] = scaling_factor.text\n",
    "                if eicv is not None:\n",
    "                    assessor_data['eICV'] = eicv.text\n",
    "            \n",
    "            # Segmentation data\n",
    "            if 'segmentationFastData' in assessor_type:\n",
    "                brain_percent = assessor.get('brainPercent')\n",
    "                if brain_percent:\n",
    "                    assessor_data['brain_percent'] = brain_percent\n",
    "            \n",
    "            data['assessors'].append(assessor_data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process All Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_oasis_dataset(data_dir):\n",
    "    \"\"\"Process all OASIS dataset files and create dataframes.\"\"\"\n",
    "    \n",
    "    subjects_data = []\n",
    "    scans_data = []\n",
    "    assessors_data = []\n",
    "    \n",
    "    # Find all disc directories\n",
    "    data_path = Path(data_dir)\n",
    "    disc_dirs = [d for d in data_path.iterdir() if d.is_dir() and 'disc' in d.name.lower()]\n",
    "    \n",
    "    for disc_dir in disc_dirs:\n",
    "        print(f\"Processing {disc_dir.name}...\")\n",
    "        \n",
    "        # Navigate to the actual disc folder (e.g., disc1, disc2)\n",
    "        disc_folders = [d for d in disc_dir.iterdir() if d.is_dir() and d.name.startswith('disc')]\n",
    "        \n",
    "        for disc_folder in disc_folders:\n",
    "            # Find all subject directories (OAS1_XXXX_MR1)\n",
    "            subject_dirs = [d for d in disc_folder.iterdir() if d.is_dir() and d.name.startswith('OAS1_')]\n",
    "            \n",
    "            for subject_dir in subject_dirs:\n",
    "                subject_id = subject_dir.name\n",
    "                txt_file = subject_dir / f\"{subject_id}.txt\"\n",
    "                xml_file = subject_dir / f\"{subject_id}.xml\"\n",
    "                \n",
    "                if txt_file.exists() and xml_file.exists():\n",
    "                    # Parse txt file\n",
    "                    txt_data = parse_txt_file(txt_file)\n",
    "                    \n",
    "                    # Parse xml file\n",
    "                    xml_data = parse_xml_file(xml_file)\n",
    "                    \n",
    "                    # Combine subject data\n",
    "                    subject_record = {**txt_data, 'disc': disc_dir.name}\n",
    "                    subjects_data.append(subject_record)\n",
    "                    \n",
    "                    # Add scans data\n",
    "                    for scan in xml_data['scans']:\n",
    "                        scan_record = {'subject_id': subject_id, **scan}\n",
    "                        scans_data.append(scan_record)\n",
    "                    \n",
    "                    # Add assessors data\n",
    "                    for assessor in xml_data['assessors']:\n",
    "                        assessor_record = {'subject_id': subject_id, **assessor}\n",
    "                        assessors_data.append(assessor_record)\n",
    "    \n",
    "    # Create DataFrames\n",
    "    df_subjects = pd.DataFrame(subjects_data)\n",
    "    df_scans = pd.DataFrame(scans_data)\n",
    "    df_assessors = pd.DataFrame(assessors_data)\n",
    "    \n",
    "    return df_subjects, df_scans, df_assessors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the data directory path\n",
    "data_dir = './data'\n",
    "\n",
    "# Process the dataset\n",
    "df_subjects, df_scans, df_assessors = process_oasis_dataset(data_dir)\n",
    "\n",
    "print(f\"\\nProcessing complete!\")\n",
    "print(f\"Subjects: {len(df_subjects)} records\")\n",
    "print(f\"Scans: {len(df_scans)} records\")\n",
    "print(f\"Assessors: {len(df_assessors)} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Explore the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display subjects dataframe\n",
    "print(\"\\n=== SUBJECTS DATAFRAME ===\")\n",
    "print(df_subjects.head())\n",
    "print(f\"\\nShape: {df_subjects.shape}\")\n",
    "print(f\"\\nColumns: {df_subjects.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scans dataframe\n",
    "print(\"\\n=== SCANS DATAFRAME ===\")\n",
    "print(df_scans.head())\n",
    "print(f\"\\nShape: {df_scans.shape}\")\n",
    "print(f\"\\nColumns: {df_scans.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display assessors dataframe\n",
    "print(\"\\n=== ASSESSORS DATAFRAME ===\")\n",
    "print(df_assessors.head())\n",
    "print(f\"\\nShape: {df_assessors.shape}\")\n",
    "print(f\"\\nColumns: {df_assessors.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Type Conversion and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric columns to appropriate types\n",
    "numeric_cols = ['AGE', 'EDUC', 'SES', 'CDR', 'MMSE', 'eTIV', 'ASF', 'nWBV']\n",
    "for col in numeric_cols:\n",
    "    if col in df_subjects.columns:\n",
    "        df_subjects[col] = pd.to_numeric(df_subjects[col], errors='coerce')\n",
    "\n",
    "# Convert scan parameters to numeric\n",
    "scan_numeric_cols = ['voxel_x', 'voxel_y', 'voxel_z', 'tr', 'te', 'ti', 'flip']\n",
    "for col in scan_numeric_cols:\n",
    "    if col in df_scans.columns:\n",
    "        df_scans[col] = pd.to_numeric(df_scans[col], errors='coerce')\n",
    "\n",
    "# Convert assessor values to numeric\n",
    "assessor_numeric_cols = ['scaling_factor', 'eICV', 'brain_percent']\n",
    "for col in assessor_numeric_cols:\n",
    "    if col in df_assessors.columns:\n",
    "        df_assessors[col] = pd.to_numeric(df_assessors[col], errors='coerce')\n",
    "\n",
    "print(\"Data types converted successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject demographics\n",
    "print(\"\\n=== SUBJECT DEMOGRAPHICS ===\")\n",
    "print(f\"\\nAge statistics:\")\n",
    "print(df_subjects['AGE'].describe())\n",
    "\n",
    "print(f\"\\nGender distribution:\")\n",
    "print(df_subjects['M/F'].value_counts())\n",
    "\n",
    "print(f\"\\nCDR (Clinical Dementia Rating) distribution:\")\n",
    "print(df_subjects['CDR'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save DataFrames to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV files\n",
    "df_subjects.to_csv('oasis_subjects.csv', index=False)\n",
    "df_scans.to_csv('oasis_scans.csv', index=False)\n",
    "df_assessors.to_csv('oasis_assessors.csv', index=False)\n",
    "\n",
    "print(\"\\nDataFrames saved to CSV files:\")\n",
    "print(\"- oasis_subjects.csv\")\n",
    "print(\"- oasis_scans.csv\")\n",
    "print(\"- oasis_assessors.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
